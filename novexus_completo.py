# -*- coding: utf-8 -*-
"""Novexus (Semana 1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cctn090CKzR_dK05Lu3ZNwliCHCbQfYx
"""

from imblearn.under_sampling import RandomUnderSampler
from pandas import json_normalize
from sklearn.dummy import DummyClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.metrics import recall_score
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import pickle
import plotly.express as px
import random
import seaborn as sns
import xgboost as xgb
from joblib import load
from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score
from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

"""#Funções"""

# FUNÇÃO PARA ANÁLISE CATEGÓRICA

def analise_categorica(df, coluna):

    print(f"Análise da variável categórica: {coluna}", end='\n\n')

    # Identificar e exibir os tipos de dados únicos na coluna
    tipos_unicos = df[coluna].apply(type).unique()
    print(f"Tipos de dados: {tipos_unicos}", end='\n\n')

    # Contagem de valores nulos ou ausentes
    nulos = df[coluna].isnull().sum()
    print(f"Quantidade de valores nulos ou ausentes: {nulos}", end='\n\n')

    # Limpar e padronizar strings, se forem de fato strings
    if pd.api.types.is_string_dtype(df[coluna]):
        df_normalizado.loc[:, coluna] = df_normalizado[coluna].str.upper().str.strip()

    # Contagem de valores únicos
    print(f"Quantidade de valores únicos: {df[coluna].nunique()}", end='\n\n')

    # Exibir os valores únicos
    print("Valores únicos: ", df[coluna].unique(), end='\n\n')

    # Contagem de frequência dos valores
    print("Contagem de frequência dos valores:")
    contagem_frequencia = df[coluna].value_counts()
    print(contagem_frequencia, end='\n\n')

    # Criar um gráfico de barras para a contagem de classes
    plt.figure(figsize=(7, 4))
    ax = contagem_frequencia.plot(kind='bar')
    plt.xlabel(f'Classes de {coluna}')
    plt.ylabel('Contagem')
    plt.title(f'Contagem de classes da variável {coluna}')

    # Adicionar rótulos numéricos
    for i, v in enumerate(contagem_frequencia):
      ax.text(i, v + 0.2, str(v), ha='center', va='bottom')

plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Função para análise de dados numéricos contínuos
def analise_numerica(df, coluna):
    print(f"Análise da variável numérica: {coluna.upper()}", end='\n\n')

    # Verificar e exibir o tipo de dados
    tipo_dado = df[coluna].dtype
    print(f"Tipo de dado: {tipo_dado}", end='\n\n')

    # Média
    media = round(df[coluna].mean(), 2)
    print(f"Média: {media}", end='\n\n')

    # Mediana
    mediana = round(df[coluna].median(), 2)
    print(f"Mediana: {mediana}", end='\n\n')

    # Moda
    moda = round(df[coluna].mode()[0], 2)  # Pegando o primeiro valor da moda
    print(f"Moda: {moda}", end='\n\n')

    # Desvio padrão
    desvio_padrao = round(df[coluna].std(), 2)
    print(f"Desvio Padrão: {desvio_padrao}", end='\n\n')

    # Mínimo
    minimo = round(df[coluna].min(), 2)
    print(f"Valor Mínimo: {minimo}", end='\n\n')

    # Máximo
    maximo = round(df[coluna].max(), 2)
    print(f"Valor Máximo: {maximo}", end='\n\n')

    # Quartis
    q1, q3 = round(df[coluna].quantile(0.25), 2), round(df[coluna].quantile(0.75), 2)
    print(f"1º Quartil: {q1}", end='\n\n')
    print(f"3º Quartil: {q3}", end='\n\n')

    # Intervalo Interquartil (IQR)
    iqr = round(q3 - q1, 2)
    print(f"Intervalo Interquartil (IQR): {iqr}", end='\n\n')

# Função para imprimir boxplot de variáveis numéricas
def imprime_boxplot(df, coluna):

    # Definindo o tamanho da figura
    plt.figure(figsize=(8, 4))

    # Geração do boxplot
    sns.boxplot(x=df[coluna])

    # Definindo o título do gráfico
    plt.title(f'Boxplot de {coluna}')

    # Definindo o rótulo do eixo x
    plt.xlabel(coluna)

    # Exibindo o gráfico
    plt.show()

# Função para análise numérica completa
def analise_numerica_completa(df, coluna):
    # Tentativa de conversão da coluna para float
    try:
        df[coluna] = df[coluna].astype(float)
    except ValueError:
        print(f"Não foi possível converter a coluna {coluna} para um tipo numérico.")
        return

    analise_numerica(df, coluna)
    imprime_boxplot(df, coluna)


def plot_churn_by_feature(df, col_churn, col_feature, colors_sim=['darkred', 'lightcoral'], colors_nao=['darkgreen', 'lightgreen']):
    # Agrupando e contando as ocorrências
    df_grouped = df.groupby([col_churn, col_feature]).size().reset_index(name='Contagem')

    # Calculando a soma total de contagens para cada categoria de 'col_churn'
    total_counts = df_grouped.groupby(col_churn)['Contagem'].transform('sum')

    # Normalizando as contagens para obter proporções
    df_grouped['Proporcao'] = df_grouped['Contagem'] / total_counts

    # Configurações de plotagem
    fig, axs = plt.subplots(1, 2, figsize=(15, 6))

    # Dados para 'Rotatividade: SIM'
    df_sim = df_grouped[df_grouped[col_churn] == 'SIM']

    # Dados para 'Rotatividade: NÃO'
    df_nao = df_grouped[df_grouped[col_churn] == 'NÃO']

    # Plot para 'Rotatividade: SIM'
    axs[0].bar(df_sim[col_feature], df_sim['Proporcao'], color=colors_sim)
    axs[0].set_title(f'{col_churn}: SIM')
    axs[0].set_xlabel(col_feature)
    axs[0].set_ylabel('Proporção')

    # Plot para 'Rotatividade: NÃO'
    axs[1].bar(df_nao[col_feature], df_nao['Proporcao'], color=colors_nao)
    axs[1].set_title(f'{col_churn}: NÃO')
    axs[1].set_xlabel(col_feature)
    axs[1].set_ylabel('Proporção')

    # Legendas e Títulos
    for ax in axs:
        ax.legend(df_grouped[col_feature].unique())

    plt.suptitle(f'{col_churn} por {col_feature}')
    plt.show()

"""# <font color='magenta'><b>BASE DE DADOS:</b>

Churn de Clientes - Descrição
Através da API da Novexus, teremos acesso à base de dados e às suas informações.

Para isso, basta acessar a Base de dados - Novexus.

https://challenge-data-science-3ed.s3.amazonaws.com/Telco-Customer-Churn.json
"""

uri = 'https://challenge-data-science-3ed.s3.amazonaws.com/Telco-Customer-Churn.json'
dados = pd.read_json(uri)
print(type(dados))
dados.sample(1)

# percorre todas as Series do DataFrame e identifica os tipos de dados.
for coluna in dados:
    tipos_unicos = dados[coluna].apply(type).unique()
    print(f"Coluna: {coluna} - Tipos de dados: {tipos_unicos}")

"""## Normalize a base de dados."""

# Lista de colunas para normalizar
colunas_para_normalizar = ['customer', 'phone', 'internet', 'account']

# Inicializar um DataFrame para armazenar o resultado final
df_normalizado = dados.drop(colunas_para_normalizar, axis=1).copy()

# Loop através das colunas
for column in colunas_para_normalizar:
    # Aplique json_normalize para a coluna
    coluna_normalizada = json_normalize(dados[column])

    # Renomear colunas do DataFrame aplainado para evitar conflitos de nome
    coluna_normalizada.columns = [f"{column}_{subcol}" for subcol in coluna_normalizada.columns]

    # Concatene o DataFrame aplainado ao DataFrame final
    df_normalizado = pd.concat([df_normalizado, coluna_normalizada], axis=1)

pd.set_option('display.max_columns', 50)

# Exclui a series 'customerID'
df_normalizado = df_normalizado.drop('customerID', axis=1)

# Exibe as primeiras linhas do DataFrame final
df_normalizado.head()

"""## Utilize métodos que fornecem descrições estatísticas e resumo dos dados."""

df_normalizado.columns

"""#### Churn: se o cliente deixou ou não a empresa - <font color='yellow'>variável alvo"""

analise_categorica(df_normalizado, 'Churn')

# Criar um novo DataFrame contendo apenas as linhas onde 'Churn' é diferente de 'YES' ou 'NO'
df_no_churn = df_normalizado[(df_normalizado['Churn'] != 'YES') & (df_normalizado['Churn'] != 'NO')]

# Remover essas linhas do DataFrame original
df_normalizado = df_normalizado[(df_normalizado['Churn'] == 'YES') | (df_normalizado['Churn'] == 'NO')]

analise_categorica(df_normalizado, 'Churn')

"""#### gender: gênero (masculino e feminino)"""

analise_categorica(df_normalizado, 'customer_gender')

"""####SeniorCitizen: informação sobre um cliente ter ou não idade igual ou maior que 65 anos"""

analise_categorica(df_normalizado, 'customer_SeniorCitizen')

"""#### Partner: se o cliente possui ou não um parceiro ou parceira"""

analise_categorica(df_normalizado, 'customer_Partner')

"""#### Dependents: se o cliente possui ou não dependentes"""

analise_categorica(df_normalizado, 'customer_Dependents')

"""#### tenure: meses de contrato do cliente"""

analise_numerica_completa(df_normalizado, 'customer_tenure')

"""####PhoneService: assinatura de serviço telefônico"""

analise_categorica(df_normalizado, 'phone_PhoneService')

"""#### MultipleLines: assinatura de mais de uma linha de telefone"""

analise_categorica(df_normalizado, 'phone_MultipleLines')

# Substituir 'NO PHONE SERVICE' por 'NO'
df_normalizado['phone_MultipleLines'].replace('NO PHONE SERVICE', 'NO', inplace=True)

analise_categorica(df_normalizado, 'phone_MultipleLines')

"""#### InternetService: assinatura de um provedor de internet"""

analise_categorica(df_normalizado, 'internet_InternetService')

"""Excluir a Series 'NO' que será criada após o one_hot.

####OnlineSecurity: assinatura adicional de segurança online
"""

analise_categorica(df_normalizado, 'internet_OnlineSecurity')

# Exclui os registros da categoria NO INTERNET SERVICE, pois já existe a feature.
df_normalizado = df_normalizado.loc[df_normalizado['internet_OnlineSecurity'] != 'NO INTERNET SERVICE']

analise_categorica(df_normalizado, 'internet_OnlineSecurity')

"""####OnlineBackup: assinatura adicional de backup online"""

analise_categorica(df_normalizado, 'internet_OnlineBackup')

"""#### DeviceProtection: assinatura adicional de proteção no dispositivo"""

analise_categorica(df_normalizado, 'internet_DeviceProtection')

"""#### TechSupport: assinatura adicional de suporte técnico, menos tempo de espera"""

analise_categorica(df_normalizado, 'internet_TechSupport')

"""#### StreamingTV: assinatura de TV a cabo"""

analise_categorica(df_normalizado, 'internet_StreamingTV')

"""#### StreamingMovies: assinatura de streaming de filmes"""

analise_categorica(df_normalizado, 'internet_StreamingMovies')

"""####Contract: tipo de contrato"""

analise_categorica(df_normalizado, 'account_Contract')

"""#### PaperlessBilling: se o cliente prefere receber online a fatura"""

analise_categorica(df_normalizado, 'account_PaperlessBilling')

"""#### PaymentMethod: forma de pagamento"""

analise_categorica(df_normalizado, 'account_PaymentMethod')

"""#### Charges.Monthly: total de todos os serviços do cliente por mês"""

analise_numerica_completa(df_normalizado, 'account_Charges.Monthly')

"""#### Charges.Total: total gasto pelo cliente - (var numérica)"""

analise_numerica_completa(df_normalizado, 'account_Charges.Total')

# Identificar e exibir os tipos de dados únicos na coluna
tipos_unicos = df_normalizado['account_Charges.Total'].apply(type).unique()
print(f"Tipos de dados: {tipos_unicos}", end='\n\n')

# Contagem de valores nulos ou ausentes
nulos = df_normalizado['account_Charges.Total'].isnull().sum()
print(f"Quantidade de valores nulos ou ausentes: {nulos}", end='\n\n')

# Limpar e padronizar strings, se forem de fato strings
if pd.api.types.is_string_dtype(df_normalizado['account_Charges.Total']):
  df_normalizado['account_Charges.Total'] = df_normalizado['account_Charges.Total'].str.upper().str.strip()

# Contagem de valores únicos
print(f"Quantidade de valores únicos: {df_normalizado['account_Charges.Total'].nunique()}", end='\n\n')

# Exibir os valores únicos
print("Valores únicos: ", df_normalizado['account_Charges.Total'].unique(), end='\n\n')

# Contagem de frequência dos valores
print("Contagem de frequência dos valores:")
contagem_frequencia = df_normalizado['account_Charges.Total'].value_counts()
print(contagem_frequencia, end='\n\n')

# Identifica os valores que não podem ser convertidos para float
nao_numericos = df_normalizado['account_Charges.Total'].apply(lambda x: not x.replace('.', '', 1).isdigit())
print(len(df_normalizado[nao_numericos]))
print(df_normalizado[nao_numericos])

"""<font color='yellow'>Existem valores em branco, não nulos, na coluna account_Charges.Total."""

# converte a coluna para numérico e erros em NaN
df_normalizado['account_Charges.Total'] = pd.to_numeric(df_normalizado['account_Charges.Total'], errors='coerce')

# Contagem de valores nulos ou ausentes
nulos = df_normalizado['account_Charges.Total'].isnull().sum()
print(f"Quantidade de valores nulos ou ausentes: {nulos}")

# exclui os registros NaN
df_normalizado.dropna(subset=['account_Charges.Total'], inplace=True)
analise_numerica_completa(df_normalizado, 'account_Charges.Total')

"""## Confira os valores únicos de cada coluna."""

for coluna in df_normalizado.columns:
        valores_unicos = df_normalizado[coluna].unique()
        quantidade_valores_unicos = len(valores_unicos)

        print(f"Coluna: {coluna}")
        print(f"Quantidade de valores únicos: {quantidade_valores_unicos}")
        print(f"Valores únicos: {valores_unicos}")
        print("-" * 40)  # separador

"""## Analisar a tipagem dos dados obtidos."""

# percorre todas as Series do DataFrame e identifica os tipos de dados.
for coluna in df_normalizado:
    tipos_unicos = df_normalizado[coluna].apply(type).unique()
    print(f"Coluna: {coluna} - Tipos de dados: {tipos_unicos}")

"""# <h1> <font color='red'>DESAFIO:</font> Entender quais informações o conjunto de dados possui.

# <h1> <font color='red'>DESAFIO:</font> Verificar quais são as inconsistências nos dados

## Conferir se há nulos no conjunto de dados e fazer o tratamento adequadas.
"""

import pandas as pd
for coluna in df_normalizado.columns:
    nulos = df_normalizado[coluna].isna().sum()
    espacos_em_branco = df_normalizado[coluna].apply(lambda x: str(x).isspace()).sum()

    print(f"Coluna: {coluna}")
    print(f"Valores nulos ou ausentes: {nulos}")
    print(f"Campos preenchidos apenas com espaço: {espacos_em_branco}")
    print("-" * 40)  # separador

"""As correções crísticas, como tratamento de dados ausentes, foram realizadas durante a análise exploratória individual de cada feature."""

df_normalizado.head()

"""# <h1><font color='red'> DESAFIO:</font> Criar visualizações relevantes em relação ao Churn

Nesta parte é interessante focar em quais visualizações serão melhores para apresentar, e para isso podemos ver o vídeo Data Visualization: tipos de visualização. Os gráficos podem ser feitos usando o Matplotlib, Seaborn ou alguma outra biblioteca gráfica da sua escolha.

Algumas das análises gráficas que podem ser realizadas são:

Funções
"""

import plotly.express as px

def criar_grafico_barras_empilhadas(df, coluna_x, coluna_cor, titulo, rotulo_y):
    # Agrupar os dados pelas colunas especificadas e contar a frequência
    df_agrupado = df.groupby([coluna_x, coluna_cor]).size().reset_index(name='count')

    # Criar o gráfico de barras empilhadas
    fig = px.bar(df_agrupado, x=coluna_x, y='count', color=coluna_cor,
                 title=titulo, labels={'count': rotulo_y})

    # Mostrar o gráfico
    fig.show()


def criar_grafico_boxplot(df, coluna_numerica, coluna_categorica, titulo):
    # Criar o gráfico de boxplot
    fig = px.box(df, x=coluna_categorica, y=coluna_numerica,
                 title=titulo, labels={coluna_numerica: 'Customer Tenure', coluna_categorica: 'Churn'})

    # Mostrar o gráfico
    fig.show()

"""###Distribuição da variável gênero por Churn"""

criar_grafico_barras_empilhadas(df_normalizado, 'customer_gender', 'Churn',
                                'Comparação de Churn por Gênero do Cliente',
                                'Quantidade de Clientes')

"""###Distribuição da variável InternetService por Churn"""

criar_grafico_barras_empilhadas(df_normalizado, 'internet_InternetService', 'Churn',
                                'Comparação de Churn por Tipo de Serviço de Internet',
                                'Quantidade de Clientes')

"""###Distribuição da variável OnlineSecurity por Churn"""

criar_grafico_barras_empilhadas(df_normalizado, 'internet_OnlineSecurity', 'Churn',
                                'Comparação de Churn por internet_OnlineSecurity',
                                'Quantidade de Clientes')

"""###Distribuição da variável DeviceProtection por Churn"""

criar_grafico_barras_empilhadas(df_normalizado, 'internet_DeviceProtection', 'Churn',
                                'Comparação de Churn por internet_DeviceProtection',
                                'Quantidade de Clientes')

"""###Distribuição da variável TechSupport por Churn"""

criar_grafico_barras_empilhadas(df_normalizado, 'internet_TechSupport', 'Churn',
                                'Comparação de Churn por internet_TechSupport',
                                'Quantidade de Clientes')

"""###Distribuição da variável Contract por Churn"""

criar_grafico_barras_empilhadas(df_normalizado, 'account_Contract', 'Churn',
                                'Comparação de Churn por account_Contract',
                                'Quantidade de Clientes')

"""###Distribuição da variável PaymentMethod por Churn"""

criar_grafico_barras_empilhadas(df_normalizado, 'account_PaymentMethod', 'Churn',
                                'Comparação de Churn por account_PaymentMethod',
                                'Quantidade de Clientes')

"""###Distribuição da variável SeniorCitizen por Churn"""

criar_grafico_barras_empilhadas(df_normalizado, 'customer_SeniorCitizen', 'Churn',
                                'Comparação de Churn por customer_SeniorCitizen',
                                'Quantidade de Clientes')

"""###Boxplot para as variáveis numéricas"""

# Exemplo de uso da função
criar_grafico_boxplot(df_normalizado, 'customer_tenure', 'Churn',
                      'Comparação de Customer Tenure por Churn')

# Exemplo de uso da função
criar_grafico_boxplot(df_normalizado, 'account_Charges.Monthly', 'Churn',
                      'Comparação de account_Charges.Monthly por Churn')

# Exemplo de uso da função
criar_grafico_boxplot(df_normalizado, 'account_Charges.Total', 'Churn',
                      'Comparação de account_Charges.Total por Churn')

"""# <h1><font color='red'> DESAFIO:</font> Aplicar encoding adequado nos dados

Realize a tranformações da variáveis que são categóricas com mais de duas categorias com o encoder correto.

## customer_gender
"""

analise_categorica(df_normalizado, 'customer_gender')

# Aplicando one-hot encoding à coluna 'customer_gender'
one_hot = pd.get_dummies(df_normalizado['customer_gender'], prefix='customer_gender')

# Juntar as novas colunas ao DataFrame original e remover a coluna 'customer_gender' antiga
df_normalizado = pd.concat([df_normalizado, one_hot], axis=1)

# Exclui a coluna original
df_normalizado.drop('customer_gender', axis=1, inplace=True)

analise_categorica(df_normalizado, 'customer_gender_FEMALE')

analise_categorica(df_normalizado, 'customer_gender_MALE')

"""##customer_tenure"""

df_normalizado['customer_tenure'] = pd.to_numeric(df_normalizado['customer_tenure'], errors='coerce')

# Definindo os intervalos e labels para as categorias
bins = [0, 5, 10, 20, 30, 40, 50, 60, 70, 80]
labels = ['0-5', '6-10', '11-20', '21-30', '31-40', '41-50', '51-60', '61-70', '71-80']

# Criando uma nova coluna com as categorias
df_normalizado['tenure_group'] = pd.cut(df_normalizado['customer_tenure'], bins=bins, labels=labels, right=False)

# Aplicando one-hot encoding à coluna de categorias
df_one_hot = pd.get_dummies(df_normalizado['tenure_group'], prefix='tenure')

# Concatenando as novas colunas ao DataFrame original
df_normalizado = pd.concat([df_normalizado, df_one_hot], axis=1)

# Descartando as colunas 'customer_tenure' e 'tenure_group'
df_normalizado.drop(['customer_tenure', 'tenure_group'], axis=1, inplace=True)

analise_categorica(df_normalizado, 'tenure_0-5')

analise_categorica(df_normalizado, 'tenure_6-10')

analise_categorica(df_normalizado, 'tenure_11-20')

analise_categorica(df_normalizado, 'tenure_21-30')

analise_categorica(df_normalizado, 'tenure_31-40')

analise_categorica(df_normalizado, 'tenure_41-50')

analise_categorica(df_normalizado, 'tenure_61-70')

analise_categorica(df_normalizado, 'tenure_71-80')

"""##account_Contract"""

analise_categorica(df_normalizado, 'account_Contract')

# Aplicando one-hot encoding à coluna 'internet_InternetService'
one_hot = pd.get_dummies(df_normalizado['account_Contract'], prefix='account_Contract')

# Juntar as novas colunas ao DataFrame original e remover a coluna 'internet_InternetService' antiga
df_normalizado = pd.concat([df_normalizado, one_hot], axis=1)

# Exclui a coluna original
df_normalizado.drop('account_Contract', axis=1, inplace=True)

analise_categorica(df_normalizado, 'account_Contract_MONTH-TO-MONTH')

analise_categorica(df_normalizado, 'account_Contract_ONE YEAR')

analise_categorica(df_normalizado, 'account_Contract_TWO YEAR')

"""##account_PaymentMethod"""

analise_categorica(df_normalizado, 'account_PaymentMethod')

# Aplicando one-hot encoding à coluna 'internet_InternetService'
one_hot = pd.get_dummies(df_normalizado['account_PaymentMethod'], prefix='account_PaymentMethod')

# Juntar as novas colunas ao DataFrame original e remover a coluna 'internet_InternetService' antiga
df_normalizado = pd.concat([df_normalizado, one_hot], axis=1)

# Exclui a coluna original
df_normalizado.drop('account_PaymentMethod', axis=1, inplace=True)

analise_categorica(df_normalizado, 'account_PaymentMethod_BANK TRANSFER (AUTOMATIC)')

analise_categorica(df_normalizado, 'account_PaymentMethod_CREDIT CARD (AUTOMATIC)')

analise_categorica(df_normalizado, 'account_PaymentMethod_ELECTRONIC CHECK')

analise_categorica(df_normalizado, 'account_PaymentMethod_MAILED CHECK')

"""##account_Charges.Monthly"""

analise_numerica_completa(df_normalizado, 'account_Charges.Monthly')

bins = [0, 50, 100, 150]
labels = ['0-50', '51-100', '101-150']

# Criando uma nova coluna com as categorias
df_normalizado['monthly_charges_group'] = pd.cut(df_normalizado['account_Charges.Monthly'], bins=bins, labels=labels, right=False)

# Aplicando one-hot encoding à coluna de categorias
df_one_hot = pd.get_dummies(df_normalizado['monthly_charges_group'], prefix='monthly_charges')

# Concatenando o DataFrame original com o DataFrame de one-hot encoding
df_normalizado = pd.concat([df_normalizado, df_one_hot], axis=1)

# Excluindo as colunas originais e a coluna de categorias temporária
df_normalizado.drop(['account_Charges.Monthly', 'monthly_charges_group'], axis=1, inplace=True)

analise_categorica(df_normalizado, 'monthly_charges_0-50')

analise_categorica(df_normalizado, 'monthly_charges_51-100')

analise_categorica(df_normalizado, 'monthly_charges_101-150')

"""Todos os registros (mesmo outliers) foram incluídos na categorização da variável numérica, pois podem conter informações relevantes na detecção de Churn. Serão utilizados algoritmos mais robustos à outliers durante a modelagem.

##account_Charges.Total
"""

analise_numerica_completa(df_normalizado, 'account_Charges.Total')

# Definindo os intervalos e rótulos para as categorias
# Os intervalos são apenas um exemplo e devem ser ajustados conforme o seu entendimento dos dados e do domínio.
bins = [0, 500, 1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000]
labels = ['0-500', '501-1000', '1001-2000', '2001-3000', '3001-4000', '4001-5000', '5001-6000', '6001-7000', '7001-8000', '8001-9000']

# Criando uma nova coluna com as categorias
df_normalizado['account_charges_group'] = pd.cut(df_normalizado['account_Charges.Total'], bins=bins, labels=labels, right=False)

# Aplicando one-hot encoding à coluna de categorias
df_one_hot = pd.get_dummies(df_normalizado['account_charges_group'], prefix='account_charges')

# Concatenando as novas colunas ao DataFrame original
df_normalizado = pd.concat([df_normalizado, df_one_hot], axis=1)

# Removendo as colunas 'account_Charges.Total' e 'account_charges_group' se necessário
df_normalizado.drop(['account_Charges.Total', 'account_charges_group'], axis=1, inplace=True)

analise_categorica(df_normalizado, 'account_charges_0-500')

analise_categorica(df_normalizado, 'account_charges_501-1000')

analise_categorica(df_normalizado, 'account_charges_1001-2000')

analise_categorica(df_normalizado, 'account_charges_2001-3000')

analise_categorica(df_normalizado, 'account_charges_3001-4000')

analise_categorica(df_normalizado, 'account_charges_4001-5000')

analise_categorica(df_normalizado, 'account_charges_5001-6000')

analise_categorica(df_normalizado, 'account_charges_7001-8000')

analise_categorica(df_normalizado, 'account_charges_8001-9000')

"""##internet_InternetService"""

analise_categorica(df_normalizado, 'internet_InternetService')

# Aplicando one-hot encoding à coluna 'internet_InternetService'
one_hot = pd.get_dummies(df_normalizado['internet_InternetService'], prefix='internet_InternetService')

# Juntar as novas colunas ao DataFrame original e
df_normalizado = pd.concat([df_normalizado, one_hot], axis=1)

# Remove a coluna 'internet_InternetService' antiga
df_normalizado=df_normalizado.drop('internet_InternetService', axis=1)

analise_categorica(df_normalizado, 'internet_InternetService_DSL')

analise_categorica(df_normalizado, 'internet_InternetService_FIBER OPTIC')


replacement_dict = {'NO': 0, 'YES': 1}
df_normalizado = df_normalizado.replace(replacement_dict)

from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2

# CARACTERÍSTICAS
X = df_normalizado.drop('Churn', axis=1)

# TARGET
y = df_normalizado['Churn']

# Aplicando o teste chi2
chi2_selector = SelectKBest(chi2, k=4)  # Seleciona as 4 melhores características
X_kbest = chi2_selector.fit_transform(X, y)

# Obtenha os nomes das colunas selecionadas
important_features_chi2 = [X.columns[i] for i in chi2_selector.get_support(indices=True)]

# Crie um novo DataFrame apenas com as características importantes
X_important_chi2 = X[important_features_chi2]

# Se você também quiser incluir a coluna alvo (target) no novo DataFrame
df_important_chi2 = X_important_chi2.copy()
df_important_chi2['Churn'] = y  # Adicionando a coluna de target de volta ao novo DataFrame

df_important_chi2.columns

df_important_chi2

# Exportando uma cópia do DataFrame criado utilizando apenas as features selecionadas por chi2
df_important_chi2.to_csv('df_chi2.csv', index=False)

# Exportando uma cópia do DataFrame completo
df_normalizado.to_csv('df_completo.csv', index=False)

print("Características selecionadas:", important_features_chi2)
print("DataFrame com características importantes:", df_important_chi2.head())
print("Array de suporte:", chi2_selector.get_support())

####################################################################################################################################################################################

# -*- coding: utf-8 -*-
"""Novexus (Semana 2).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PpM1zyXUbeOqTDjZdlTAgHWjWQ2Ly-fX
"""
df = df_important_chi2


df.head()

"""# <h1> <font color='red'>DESAFIO:</font> Verificar se a variável target está balanceada.

Verifique se o conjunto de dados está balanceado. No caso do desbalanceamento, escolha um método para tratá-lo. O artigo “Lidando com o desbalanceamento de dados“ oferece informações a respeito das formas de trabalhar com dados desbalanceados.
"""

import matplotlib.pyplot as plt
import pandas as pd
import random

# Contagem das classes da variável 'Rotatividade'
class_count = df['Churn'].value_counts().sort_index()

# Plotando o gráfico de barras
plt.bar(class_count.index, class_count.values, color=['blue', 'orange'], alpha=0.7)
plt.xlabel('Classes CHURN')
plt.ylabel('Contagem')
plt.title('Distribuição da Variável CHURN')
plt.xticks(class_count.index, ['Classe 0', 'Classe 1'])
plt.show()

df['Churn'].value_counts().sort_index()

from imblearn.under_sampling import RandomUnderSampler
import pandas as pd

# Suponhamos que 'X' são suas características e 'y' são suas etiquetas (classes)
X = df.drop('Churn', axis=1)
y = df['Churn']

# Instanciando o objeto RandomUnderSampler
rus = RandomUnderSampler(random_state=42)

# Aplicando o balanceamento
X_resampled, y_resampled = rus.fit_resample(X, y)

# Convertendo os dados balanceados de volta para um DataFrame, se necessário
X_resampled = pd.DataFrame(X_resampled, columns=df.columns[:-1])
y_resampled = pd.Series(y_resampled, name='Churn')

# Cria um novo DataFrame com os dados balanceados.
df_balanceado = pd.concat([X_resampled, y_resampled], axis=1)

# Agora, 'X_resampled' e 'y_resampled' são suas características e etiquetas balanceadas.

import matplotlib.pyplot as plt
import pandas as pd
import random

# Contagem das classes da variável 'Rotatividade'
class_count = df_balanceado['Churn'].value_counts().sort_index()

# Plotando o gráfico de barras
plt.bar(class_count.index, class_count.values, color=['blue', 'orange'], alpha=0.7)
plt.xlabel('Classes CHURN')
plt.ylabel('Contagem')
plt.title('Distribuição da Variável CHURN após balanceamento')
plt.xticks(class_count.index, ['Classe 0', 'Classe 1'])
plt.show()

df.to_csv('df_balanceado.csv', index=False)

"""# <h1> <font color='blue'>EXTRA:</font> Qual métrica otimizar?

Analise o problema que você está resolvendo e com base nos conteúdos aprendidos escolha a métrica de classificação ideal para ser otimizada.

Recall

# <h1> <font color='red'>DESAFIO:</font> Criar dois ou mais modelos de Machine Learning.

Para melhorar o projeto é aconselhável que você implemente pelo menos dois modelos de machine learning e realize experimentações para avaliar qual modelo melhor se ajusta aos dados de acordo com as métricas de classificação. A fase de experimentação é crucial para o processo e pode te ajudar alcançar melhores resultados.

Dica: que tal experimentar diferentes modelos com metodologias distintas? Por exemplo, modelos baseados em bagging e modelos que utilizam boosting. Dessa forma, você pode avaliar qual abordagem funciona melhor para o seu caso.

Modelos:

* Random Forest
* XGBoost
* Regressão Logística

##<h1>Analisando o desempenho dos modelos no conjunto de dados com todas as features
"""

from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.dummy import DummyClassifier
from sklearn.linear_model import LogisticRegression
import xgboost as xgb

df_completo = df_normalizado

# Suponhamos que 'X' são suas características e 'y' são suas etiquetas (classes)
X = df_completo.drop('Churn', axis=1)
y = df_completo['Churn']

# Criar os objetos de classificação
clf_rf = RandomForestClassifier(n_estimators=100, random_state=42)
clf_xgb = xgb.XGBClassifier(n_estimators=100, random_state=42, eval_metric='logloss')
clf_lr = LogisticRegression(max_iter=1000, random_state=42)

# Executar validação cruzada no conjunto de dados de treinamento
scores_rf_completo = cross_val_score(clf_rf, X, y, cv=5, scoring='recall')
scores_xgb_completo = cross_val_score(clf_xgb, X, y, cv=5, scoring='recall')
scores_lr_completo = cross_val_score(clf_lr, X, y, cv=5, scoring='recall')

print("Recall obtido em cada iteração da validação cruzada com Random Forest:", scores_rf_completo)
print("Recall médio na validação cruzada com Random Forest:", scores_rf_completo.mean())

print("Recall obtido em cada iteração da validação cruzada com XGBoost:", scores_xgb_completo)
print("Recall médio na validação cruzada com XGBoost:", scores_xgb_completo.mean())

print("Recall obtido em cada iteração da validação cruzada com Regressão Logística:", scores_lr_completo)
print("Recall médio na validação cruzada com Regressão Logística:", scores_lr_completo.mean())

"""##<h1>Analisando o desempenho dos modelos no conjunto de dados com as features selecionadas pelo método Chi2, mas com os dados desbalanceados."""

from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.dummy import DummyClassifier
from sklearn.linear_model import LogisticRegression
import xgboost as xgb

# Suponhamos que 'X' são suas características e 'y' são suas etiquetas (classes)
X = df.drop('Churn', axis=1)
y = df['Churn']

# Criar os objetos de classificação
clf_rf = RandomForestClassifier(n_estimators=100, random_state=42)
clf_xgb = xgb.XGBClassifier(n_estimators=100, random_state=42, eval_metric='logloss')
clf_lr = LogisticRegression(max_iter=1000, random_state=42)

# Executar validação cruzada no conjunto de dados de treinamento
scores_rf_desb = cross_val_score(clf_rf, X, y, cv=5, scoring='recall')
scores_xgb_desb = cross_val_score(clf_xgb, X, y, cv=5, scoring='recall')
scores_lr_desb = cross_val_score(clf_lr, X, y, cv=5, scoring='recall')

print("Recall obtido em cada iteração da validação cruzada com Random Forest:", scores_rf_desb)
print("Recall médio na validação cruzada com Random Forest:", scores_rf_desb.mean())

print("Recall obtido em cada iteração da validação cruzada com XGBoost:", scores_xgb_desb)
print("Recall médio na validação cruzada com XGBoost:", scores_xgb_desb.mean())

print("Recall obtido em cada iteração da validação cruzada com Regressão Logística:", scores_lr_desb)
print("Recall médio na validação cruzada com Regressão Logística:", scores_lr_desb.mean())

"""##<h1>Analisando o desempenho dos modelos no conjunto de dados com as features selecionadas pelo método Chi2 e com os dados balanceados."""

from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.dummy import DummyClassifier
from sklearn.linear_model import LogisticRegression
import xgboost as xgb

# Suponhamos que 'X' são suas características e 'y' são suas etiquetas (classes)
X = df_balanceado.drop('Churn', axis=1)
y = df_balanceado['Churn']

# Criar os objetos de classificação
clf_rf = RandomForestClassifier(n_estimators=100, random_state=42)
clf_xgb = xgb.XGBClassifier(n_estimators=100, random_state=42, eval_metric='logloss')
clf_lr = LogisticRegression(max_iter=1000, random_state=42)

# Executar validação cruzada no conjunto de dados de treinamento
scores_rf_bal = cross_val_score(clf_rf, X, y, cv=5, scoring='recall')
scores_xgb_bal = cross_val_score(clf_xgb, X, y, cv=5, scoring='recall')
scores_lr_bal = cross_val_score(clf_lr, X, y, cv=5, scoring='recall')

print("Recall obtido em cada iteração da validação cruzada com Random Forest:", scores_rf_bal)
print("Recall médio na validação cruzada com Random Forest:", scores_rf_bal.mean())

print("Recall obtido em cada iteração da validação cruzada com XGBoost:", scores_xgb_bal)
print("Recall médio na validação cruzada com XGBoost:", scores_xgb_bal.mean())

print("Recall obtido em cada iteração da validação cruzada com Regressão Logística:", scores_lr_bal)
print("Recall médio na validação cruzada com Regressão Logística:", scores_lr_bal.mean())

"""#<h1>Comparando resultados"""

import matplotlib.pyplot as plt
import numpy as np

# Armazenar os resultados em uma lista
resultados = [scores_rf_bal, scores_xgb_bal, scores_lr_bal, scores_rf_desb, scores_xgb_desb, scores_lr_desb, scores_rf_completo, scores_xgb_completo, scores_lr_completo]

# Calcular a média dos resultados para cada modelo
medias = [np.mean(scores) for scores in resultados]

# Rótulos para os modelos
modelos = ['Random Forest (Balanceado)', 'XGBoost (Balanceado)', 'Regressão Logística (Balanceado)',
           'Random Forest (Desbalanceado)', 'XGBoost (Desbalanceado)', 'Regressão Logística (Desbalanceado)',
           'Random Forest (Completo)', 'XGBoost (Completo)', 'Regressão Logística (Completo)']

# Ordenar os resultados do maior para o menor e os rótulos correspondentes
medias_ordenadas, modelos_ordenados = zip(*sorted(zip(medias, modelos), reverse=True))

# Criar o gráfico de barras ordenado
plt.figure(figsize=(14, 7))
plt.barh(modelos_ordenados, medias_ordenadas, color='skyblue')
plt.xlabel('Média dos Escores')
plt.title('Desempenho dos Modelos (Ordenado)')

# Adicionar rótulos de texto nas barras
for index, value in enumerate(medias_ordenadas):
    plt.text(value, index, f'{value:.4f}')

plt.show()

"""# <h1> <font color='red'>DESAFIO:</font> Escolher o melhor modelo

Para escolher o melhor modelo dentre os experimentados, é necessário realizar uma análise criteriosa das métricas recebidas por cada um. É importante avaliar cuidadosamente qual métrica é a mais adequada para o problema que está sendo resolvido, a fim de determinar qual modelo é o mais indicado.

Os 3 modelos tiveram um resultado excelente no conjunto de dados balanceado.

# <h1> <font color='red'>DESAFIO:</font> Otimizar o melhor modelo!

Ótimo, você já possui um modelo de machine learning! Para melhorá-lo ainda mais, sugiro que faça a otimização dos hiperparâmetros do modelo. Isso pode ajudar a maximizar o desempenho e a precisão do seu modelo.
"""

from sklearn.model_selection import GridSearchCV

# Suponhamos que 'X' são suas características e 'y' são suas etiquetas (classes)
X = df_balanceado.drop('Churn', axis=1)
y = df_balanceado['Churn']

# Parâmetros que você gostaria de otimizar para cada modelo
param_grid_rf = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10]
}

param_grid_xgb = {
    'n_estimators': [50, 100, 200],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 4, 5]
}

param_grid_lr = {
    'C': [0.001, 0.01, 0.1, 1, 10],
    'penalty': ['l1', 'l2'],
    'solver': ['newton-cg', 'lbfgs', 'liblinear']
}

# Criar os objetos GridSearchCV
grid_search_rf = GridSearchCV(estimator=clf_rf, param_grid=param_grid_rf,
                              cv=5, scoring='recall', n_jobs=-1, verbose=2)

grid_search_xgb = GridSearchCV(estimator=clf_xgb, param_grid=param_grid_xgb,
                               cv=5, scoring='recall', n_jobs=-1, verbose=2)

grid_search_lr = GridSearchCV(estimator=clf_lr, param_grid=param_grid_lr,
                              cv=5, scoring='recall', n_jobs=-1, verbose=2)

"""Hiperparâmetros Random Forest"""

# Treinar o modelo RandomForest com GridSearchCV
grid_search_rf.fit(X, y)

# Para o modelo Random Forest
best_params_rf = grid_search_rf.best_params_
best_score_rf = grid_search_rf.best_score_

print(best_params_rf)
print(best_score_rf)

"""Hiperparâmetros XGBoost"""

# Treinar o modelo XGBoost com GridSearchCV
grid_search_xgb.fit(X, y)

# Para o modelo XGBoost
best_params_xgb = grid_search_xgb.best_params_
best_score_xgb = grid_search_xgb.best_score_

print(best_params_xgb)
print(best_score_xgb)

"""Hiperparâmetros Regressão Logística"""

# Grid separado para o solver lbfgs e newton-cg que só suportam a penalidade l2
param_grid_lr_l2 = {
    'C': [0.001, 0.01, 0.1, 1, 10],
    'penalty': ['l2'],
    'solver': ['newton-cg', 'lbfgs']
}

# Grid separado para o solver liblinear que suporta as penalidades l1 e l2
param_grid_lr_l1_l2 = {
    'C': [0.001, 0.01, 0.1, 1, 10],
    'penalty': ['l1', 'l2'],
    'solver': ['liblinear']
}

# Criar os objetos GridSearchCV
grid_search_lr_l2 = GridSearchCV(estimator=clf_lr, param_grid=param_grid_lr_l2,
                                 cv=5, scoring='recall', n_jobs=-1, verbose=2)
grid_search_lr_l1_l2 = GridSearchCV(estimator=clf_lr, param_grid=param_grid_lr_l1_l2,
                                    cv=5, scoring='recall', n_jobs=-1, verbose=2)

# Treinar os modelos
grid_search_lr_l2.fit(X, y)
grid_search_lr_l1_l2.fit(X, y)

# Obter os melhores parâmetros e scores
best_params_lr_l2 = grid_search_lr_l2.best_params_
best_score_lr_l2 = grid_search_lr_l2.best_score_

best_params_lr_l1_l2 = grid_search_lr_l1_l2.best_params_
best_score_lr_l1_l2 = grid_search_lr_l1_l2.best_score_

print(best_params_lr_l2)
print(best_score_lr_l2)
print(best_params_lr_l1_l2)
print(best_score_lr_l1_l2)

import matplotlib.pyplot as plt
import numpy as np

# Preparar os dados para o gráfico
labels = ['Random Forest', 'XGBoost', 'Logistic Regression (L2)', 'Logistic Regression (L1/L2)']
scores = [best_score_rf, best_score_xgb, best_score_lr_l2, best_score_lr_l1_l2]

# Criar o gráfico de barras
plt.figure(figsize=(12, 6))
bars = plt.barh(labels, scores, color='dodgerblue')

# Adicionar rótulos de texto nas barras
for bar in bars:
    plt.text(bar.get_width() - 0.02, bar.get_y() + bar.get_height()/2 - 0.1,
             f"{bar.get_width():.6f}", va='center', ha='center', color='white', fontsize=12)

# Adicionar títulos e rótulos
plt.xlabel('Recall Score')
plt.ylabel('Model')
plt.title('Comparison of Best Recall Scores Across Models')
plt.xlim([0.87 , 0.89])  # Definindo o limite do eixo x para começar em 0.8

# Mostrar o gráfico
plt.show()

"""# <h1> <font color='blue'>EXTRA:</font> Verificar quais os hiperparâmetros escolhidos.

Uma boa prática após a otimização é verificar os hiperparâmetros utilizados no melhor modelo. Pesquise na documentação algum método que ajude a conferir os hiperparâmetros do modelo.
"""

print(best_params_lr_l1_l2)
print(best_score_lr_l1_l2)

"""# <h1> <font color='blue'>EXTRA:</font> Salvar o melhor modelo!

Salve o modelo de machine learning otimizado. Isso permitirá que o modelo seja reutilizado posteriormente para realizar previsões precisas em novos conjuntos de dados. A biblioteca pickle pode ajudar nessa tarefa!

"""

best_lr_model = grid_search_lr_l1_l2.best_estimator_  # Se você quer usar o modelo com penalidades L1 e L2
predictions_lr = best_lr_model.predict(X)

from sklearn.model_selection import cross_val_score
from sklearn.metrics import classification_report, confusion_matrix

# Avaliação com validação cruzada
cv_scores = cross_val_score(best_lr_model, X, y, cv=5, scoring='recall')

print("Recall scores com validação cruzada: ", cv_scores)
print("Média dos scores de recall: ", np.mean(cv_scores))

from sklearn.model_selection import train_test_split
from sklearn.metrics import recall_score

# Suponhamos que 'X' são suas características e 'y' são suas etiquetas (classes)
X = df_balanceado.drop('Churn', axis=1)
y = df_balanceado['Churn']

# Dividir os dados em conjuntos de treinamento e teste
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Treinar o modelo otimizado no conjunto de treinamento
best_lr_model.fit(X_train, y_train)

# Previsões no conjunto de teste
y_pred = best_lr_model.predict(X_test)

# Avaliar o modelo no conjunto de teste
print("Recall no conjunto de teste: ", recall_score(y_test, y_pred))

import pickle

# Salvar o melhor modelo de Regressão Logística utilizando pickle
with open('best_lr_model.pkl', 'wb') as model_file:
    pickle.dump(best_lr_model, model_file)

# Carregar o modelo salvo
with open('best_lr_model.pkl', 'rb') as model_file:
    loaded_model = pickle.load(model_file)

# Imprimir o modelo
print(loaded_model)

